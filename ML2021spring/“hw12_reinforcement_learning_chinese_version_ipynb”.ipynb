{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "“hw12_reinforcement_learning_chinese_version.ipynb”",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "accelerator": "GPU",
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "a3993442168a4e36a7aaf9503be3cd01": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_view_name": "HBoxView",
      "_dom_classes": [],
      "_model_name": "HBoxModel",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "box_style": "",
      "layout": "IPY_MODEL_f124e93ecf2e4c5f8d8fd762bd43824f",
      "_model_module": "@jupyter-widgets/controls",
      "children": [
       "IPY_MODEL_3b4f60be74394d3a805523bd6f547fa7",
       "IPY_MODEL_d5f7f262acf24a2faaf3cb47cb83d01a",
       "IPY_MODEL_b723620772ec456aaefd9bcf62ecc862"
      ]
     }
    },
    "f124e93ecf2e4c5f8d8fd762bd43824f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "3b4f60be74394d3a805523bd6f547fa7": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_4f945cd083aa433cb8acc84cce252f2d",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "​",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": "Total:  43.2, Final: -84.1:   1%",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_28e6008640ed42b3b2dc6fea8560f5bb"
     }
    },
    "d5f7f262acf24a2faaf3cb47cb83d01a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_view_name": "ProgressView",
      "style": "IPY_MODEL_9425467881314a15821c657b9b1179d8",
      "_dom_classes": [],
      "description": "",
      "_model_name": "FloatProgressModel",
      "bar_style": "",
      "max": 1000,
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": 12,
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "orientation": "horizontal",
      "min": 0,
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_bca0804664a9448ca3b0861b24bfb5af"
     }
    },
    "b723620772ec456aaefd9bcf62ecc862": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_view_name": "HTMLView",
      "style": "IPY_MODEL_a9923d8f5ef14968bfb79e5afdcf90dd",
      "_dom_classes": [],
      "description": "",
      "_model_name": "HTMLModel",
      "placeholder": "​",
      "_view_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "value": " 12/1000 [01:28&lt;2:14:04,  8.14s/it]",
      "_view_count": null,
      "_view_module_version": "1.5.0",
      "description_tooltip": null,
      "_model_module": "@jupyter-widgets/controls",
      "layout": "IPY_MODEL_0b47ba37ea5045e2a86be7be362124fe"
     }
    },
    "4f945cd083aa433cb8acc84cce252f2d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "28e6008640ed42b3b2dc6fea8560f5bb": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "9425467881314a15821c657b9b1179d8": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "ProgressStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "bar_color": null,
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "bca0804664a9448ca3b0861b24bfb5af": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    },
    "a9923d8f5ef14968bfb79e5afdcf90dd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_view_name": "StyleView",
      "_model_name": "DescriptionStyleModel",
      "description_width": "",
      "_view_module": "@jupyter-widgets/base",
      "_model_module_version": "1.5.0",
      "_view_count": null,
      "_view_module_version": "1.2.0",
      "_model_module": "@jupyter-widgets/controls"
     }
    },
    "0b47ba37ea5045e2a86be7be362124fe": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_view_name": "LayoutView",
      "grid_template_rows": null,
      "right": null,
      "justify_content": null,
      "_view_module": "@jupyter-widgets/base",
      "overflow": null,
      "_model_module_version": "1.2.0",
      "_view_count": null,
      "flex_flow": null,
      "width": null,
      "min_width": null,
      "border": null,
      "align_items": null,
      "bottom": null,
      "_model_module": "@jupyter-widgets/base",
      "top": null,
      "grid_column": null,
      "overflow_y": null,
      "overflow_x": null,
      "grid_auto_flow": null,
      "grid_area": null,
      "grid_template_columns": null,
      "flex": null,
      "_model_name": "LayoutModel",
      "justify_items": null,
      "grid_row": null,
      "max_height": null,
      "align_content": null,
      "visibility": null,
      "align_self": null,
      "height": null,
      "min_height": null,
      "padding": null,
      "grid_auto_rows": null,
      "grid_gap": null,
      "max_width": null,
      "order": null,
      "_view_module_version": "1.2.0",
      "grid_template_areas": null,
      "object_position": null,
      "object_fit": null,
      "grid_auto_columns": null,
      "margin": null,
      "display": null,
      "left": null
     }
    }
   }
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fp30SB4bxeQb"
   },
   "source": [
    "# **Homework 12 - Reinforcement Learning**\n",
    "\n",
    "若有任何問題，歡迎來信至助教信箱 ntu-ml-2021spring-ta@googlegroups.com\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yXsnCWPtWSNk"
   },
   "source": [
    "## 前置作業\n",
    "\n",
    "首先我們需要安裝必要的系統套件及 PyPi 套件。\n",
    "gym 這個套件由 OpenAI 所提供，是一套用來開發與比較 Reinforcement Learning 演算法的工具包（toolkit）。\n",
    "而其餘套件則是為了在 Notebook 中繪圖所需要的套件。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5e2bScpnkVbv",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "032357c8-9945-4b7c-a18e-9d07cebefc0a"
   },
   "source": [
    "!apt update\n",
    "!apt install python-opengl xvfb -y\n",
    "!pip install gym[box2d]==0.18.3 pyvirtualdisplay tqdm numpy==1.19.5 torch==1.8.1"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The operation couldn’t be completed. Unable to locate a Java Runtime that supports apt.\r\n",
      "Please visit http://www.java.com for information on installing Java.\r\n",
      "\r\n",
      "The operation couldn’t be completed. Unable to locate a Java Runtime that supports apt.\r\n",
      "Please visit http://www.java.com for information on installing Java.\r\n",
      "\r\n",
      "zsh:1: no matches found: gym[box2d]==0.18.3\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_-i3cdoYsks"
   },
   "source": [
    "接下來，設置好 virtual display，並引入所有必要的套件。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nl2nREINDLiw"
   },
   "source": [
    "%%capture\n",
    "from pyvirtualdisplay import Display\n",
    "virtual_display = Display(visible=0, size=(1400, 900))\n",
    "virtual_display.start()\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython import display\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "from tqdm.notebook import tqdm"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "ename": "EasyProcessError",
     "evalue": "start error <EasyProcess cmd_param=['Xvfb', '-help'] cmd=['Xvfb', '-help'] oserror=[Errno 2] No such file or directory: 'Xvfb' return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/easyprocess/__init__.py\u001B[0m in \u001B[0;36mstart\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    167\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 168\u001B[0;31m             self.popen = subprocess.Popen(\n\u001B[0m\u001B[1;32m    169\u001B[0m                 \u001B[0mcmd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstdout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstdout\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mstderr\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mstderr\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcwd\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcwd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/Cellar/python@3.9/3.9.6/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask)\u001B[0m\n\u001B[1;32m    950\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 951\u001B[0;31m             self._execute_child(args, executable, preexec_fn, close_fds,\n\u001B[0m\u001B[1;32m    952\u001B[0m                                 \u001B[0mpass_fds\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcwd\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0menv\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/Cellar/python@3.9/3.9.6/Frameworks/Python.framework/Versions/3.9/lib/python3.9/subprocess.py\u001B[0m in \u001B[0;36m_execute_child\u001B[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, gid, gids, uid, umask, start_new_session)\u001B[0m\n\u001B[1;32m   1820\u001B[0m                         \u001B[0merr_msg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstrerror\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0merrno_num\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1821\u001B[0;31m                     \u001B[0;32mraise\u001B[0m \u001B[0mchild_exception_type\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0merrno_num\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0merr_msg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0merr_filename\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1822\u001B[0m                 \u001B[0;32mraise\u001B[0m \u001B[0mchild_exception_type\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0merr_msg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: 'Xvfb'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mEasyProcessError\u001B[0m                          Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/cb/q0c9djs108s2csy_2320t_sr0000gp/T/ipykernel_19872/1155807123.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpyvirtualdisplay\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mDisplay\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0mvirtual_display\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mDisplay\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvisible\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1400\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m900\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0mvirtual_display\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstart\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mget_ipython\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrun_line_magic\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'matplotlib'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'inline'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/pyvirtualdisplay/display.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, backend, visible, size, color_depth, bgcolor, use_xauth, retries, extra_args, manage_global_env, **kwargs)\u001B[0m\n\u001B[1;32m     52\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"unknown backend: %s\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_backend\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 54\u001B[0;31m         self._obj = cls(\n\u001B[0m\u001B[1;32m     55\u001B[0m             \u001B[0msize\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     56\u001B[0m             \u001B[0mcolor_depth\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mcolor_depth\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/pyvirtualdisplay/xvfb.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, size, color_depth, bgcolor, use_xauth, fbdir, dpi, retries, extra_args, manage_global_env)\u001B[0m\n\u001B[1;32m     42\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dpi\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdpi\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 44\u001B[0;31m         AbstractDisplay.__init__(\n\u001B[0m\u001B[1;32m     45\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mPROGRAM\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/pyvirtualdisplay/abstractdisplay.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, program, use_xauth, retries, extra_args, manage_global_env)\u001B[0m\n\u001B[1;32m     86\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_retries_current\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     87\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 88\u001B[0;31m         \u001B[0mhelptext\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mget_helptext\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mprogram\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     89\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_has_displayfd\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m\"-displayfd\"\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mhelptext\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     90\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_has_displayfd\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/pyvirtualdisplay/util.py\u001B[0m in \u001B[0;36mget_helptext\u001B[0;34m(program)\u001B[0m\n\u001B[1;32m      8\u001B[0m     \u001B[0mp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menable_stdout_log\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m     \u001B[0mp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0menable_stderr_log\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m     \u001B[0mp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcall\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     11\u001B[0m     \u001B[0mhelptext\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstderr\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mhelptext\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/easyprocess/__init__.py\u001B[0m in \u001B[0;36mcall\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    139\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    140\u001B[0m         \"\"\"\n\u001B[0;32m--> 141\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstart\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwait\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtimeout\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    142\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_alive\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    143\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/easyprocess/__init__.py\u001B[0m in \u001B[0;36mstart\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    172\u001B[0m             \u001B[0mlog\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdebug\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"OSError exception: %s\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0moserror\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    173\u001B[0m             \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moserror\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0moserror\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 174\u001B[0;31m             \u001B[0;32mraise\u001B[0m \u001B[0mEasyProcessError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"start error\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    175\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mis_started\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    176\u001B[0m         \u001B[0mlog\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdebug\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"process was started (pid=%s)\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpid\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mEasyProcessError\u001B[0m: start error <EasyProcess cmd_param=['Xvfb', '-help'] cmd=['Xvfb', '-help'] oserror=[Errno 2] No such file or directory: 'Xvfb' return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVu9-Vdrl4E3"
   },
   "source": [
    "# 請不要更改 random seed !!!!\n",
    "# 不然在judgeboi上 你的成績不會被reproduce !!!!"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fV9i8i2YkRbO"
   },
   "source": [
    "seed = 543 # Do not change this\n",
    "def fix(env, seed):\n",
    "  env.seed(seed)\n",
    "  env.action_space.seed(seed)\n",
    "  torch.manual_seed(seed)\n",
    "  torch.cuda.manual_seed(seed)\n",
    "  torch.cuda.manual_seed_all(seed)\n",
    "  np.random.seed(seed)\n",
    "  random.seed(seed)\n",
    "  torch.set_deterministic(True)\n",
    "  torch.backends.cudnn.benchmark = False\n",
    "  torch.backends.cudnn.deterministic = True"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "He0XDx6bzjgC"
   },
   "source": [
    "最後，引入 OpenAI 的 gym，並建立一個 [Lunar Lander](https://gym.openai.com/envs/LunarLander-v2/) 環境。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "N_4-xJcbBt09"
   },
   "source": [
    "%%capture\n",
    "import gym\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "env = gym.make('LunarLander-v2')\n",
    "\n",
    "fix(env, seed)"
   ],
   "execution_count": 4,
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/cb/q0c9djs108s2csy_2320t_sr0000gp/T/ipykernel_19772/1477176722.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mgym\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mrandom\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mnumpy\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0menv\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mgym\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmake\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'LunarLander-v2'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'gym'"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "NmiAOfqRwRX5"
   },
   "source": [
    "import time\n",
    "start = time.time()"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LcMjEUWTBEEB",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "19e4e0db-2853-462c-bb91-8c73df9618ac"
   },
   "source": [
    "!pip freeze"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "addict==2.2.1\r\n",
      "appnope==0.1.2\r\n",
      "argon2-cffi==20.1.0\r\n",
      "async-generator==1.10\r\n",
      "attrs==21.2.0\r\n",
      "backcall==0.2.0\r\n",
      "beautifulsoup4==4.10.0\r\n",
      "bleach==3.3.1\r\n",
      "certifi==2021.10.8\r\n",
      "cffi==1.14.6\r\n",
      "charset-normalizer==2.0.8\r\n",
      "cycler==0.10.0\r\n",
      "debugpy==1.3.0\r\n",
      "decorator==5.0.9\r\n",
      "defusedxml==0.7.1\r\n",
      "entrypoints==0.3\r\n",
      "future==0.18.2\r\n",
      "google==3.0.0\r\n",
      "idna==3.3\r\n",
      "imageio==2.9.0\r\n",
      "imgaug==0.4.0\r\n",
      "ipykernel==6.0.2\r\n",
      "ipython==7.25.0\r\n",
      "ipython-genutils==0.2.0\r\n",
      "ipywidgets==7.6.3\r\n",
      "jedi==0.18.0\r\n",
      "Jinja2==3.0.1\r\n",
      "joblib==1.0.1\r\n",
      "jsonschema==3.2.0\r\n",
      "jupyter==1.0.0\r\n",
      "jupyter-client==6.1.12\r\n",
      "jupyter-console==6.4.0\r\n",
      "jupyter-core==4.7.1\r\n",
      "jupyterlab-pygments==0.1.2\r\n",
      "jupyterlab-widgets==1.0.0\r\n",
      "kiwisolver==1.3.1\r\n",
      "lightgbm==3.3.1\r\n",
      "line-profiler==3.2.6\r\n",
      "lmdb==1.2.1\r\n",
      "MarkupSafe==2.0.1\r\n",
      "matplotlib==3.4.2\r\n",
      "matplotlib-inline==0.1.2\r\n",
      "memory-profiler==0.58.0\r\n",
      "mistune==0.8.4\r\n",
      "mlxtend==0.19.0\r\n",
      "nbclient==0.5.3\r\n",
      "nbconvert==6.1.0\r\n",
      "nbformat==5.1.3\r\n",
      "nest-asyncio==1.5.1\r\n",
      "networkx==2.6.3\r\n",
      "notebook==6.4.0\r\n",
      "numpy==1.21.1\r\n",
      "opencv-python==4.5.3.56\r\n",
      "packaging==21.0\r\n",
      "pandas==1.3.0\r\n",
      "pandocfilters==1.4.3\r\n",
      "parso==0.8.2\r\n",
      "pexpect==4.8.0\r\n",
      "pickleshare==0.7.5\r\n",
      "Pillow==8.2.0\r\n",
      "prometheus-client==0.11.0\r\n",
      "prompt-toolkit==3.0.19\r\n",
      "psutil==5.8.0\r\n",
      "ptyprocess==0.7.0\r\n",
      "pyclipper==1.1.0.post3\r\n",
      "pycparser==2.20\r\n",
      "pydotplus==2.0.2\r\n",
      "Pygments==2.9.0\r\n",
      "pyparsing==2.4.7\r\n",
      "pyrsistent==0.18.0\r\n",
      "python-dateutil==2.8.2\r\n",
      "python-Levenshtein==0.12.2\r\n",
      "pytz==2021.1\r\n",
      "PyWavelets==1.1.1\r\n",
      "pyzmq==22.1.0\r\n",
      "qtconsole==5.1.1\r\n",
      "QtPy==1.9.0\r\n",
      "requests==2.26.0\r\n",
      "scikit-image==0.18.3\r\n",
      "scikit-learn==0.24.2\r\n",
      "scipy==1.7.0\r\n",
      "seaborn==0.11.2\r\n",
      "Send2Trash==1.7.1\r\n",
      "Shapely==1.7.1\r\n",
      "six==1.16.0\r\n",
      "soupsieve==2.2.1\r\n",
      "termcolor==1.1.0\r\n",
      "terminado==0.10.1\r\n",
      "testpath==0.5.0\r\n",
      "threadpoolctl==2.2.0\r\n",
      "tifffile==2021.8.30\r\n",
      "torch==1.9.1\r\n",
      "torchstat==0.0.7\r\n",
      "torchvision==0.10.1\r\n",
      "tornado==6.1\r\n",
      "tqdm==4.46.0\r\n",
      "traitlets==5.0.5\r\n",
      "typing-extensions==3.10.0.0\r\n",
      "urllib3==1.26.7\r\n",
      "wcwidth==0.2.5\r\n",
      "webencodings==0.5.1\r\n",
      "wget==3.2\r\n",
      "widgetsnbextension==3.5.1\r\n",
      "xgboost==1.5.0\r\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrkVvTrvWZ5H"
   },
   "source": [
    "## 什麼是 Lunar Lander？\n",
    "\n",
    "“LunarLander-v2” 這個環境是在模擬登月小艇降落在月球表面時的情形。\n",
    "這個任務的目標是讓登月小艇「安全地」降落在兩個黃色旗幟間的平地上。\n",
    "> Landing pad is always at coordinates (0,0).\n",
    "> Coordinates are the first two numbers in state vector.\n",
    "\n",
    "![](https://gym.openai.com/assets/docs/aeloop-138c89d44114492fd02822303e6b4b07213010bb14ca5856d2d49d6b62d88e53.svg)\n",
    "\n",
    "所謂的「環境」其實同時包括了 agent 和 environment。\n",
    "我們利用 `step()` 這個函式讓 agent 行動，而後函式便會回傳 environment 給予的 observation/state（以下這兩個名詞代表同樣的意思）和 reward。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bIbp82sljvAt"
   },
   "source": [
    "### Observation / State\n",
    "\n",
    "首先，我們可以看看 environment 回傳給 agent 的 observation 究竟是長什麼樣子的資料："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rsXZra3N9R5T",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e7f75466-9718-402a-dcb8-e601b0c4f514"
   },
   "source": [
    "print(env.observation_space)"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "\u001B[0;32m/var/folders/cb/q0c9djs108s2csy_2320t_sr0000gp/T/ipykernel_19772/3943762189.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0menv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mobservation_space\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mNameError\u001B[0m: name 'env' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezdfoThbAQ49"
   },
   "source": [
    "`Box(8,)` 說明我們會拿到 8 維的向量作為 observation，其中包含：垂直及水平座標、速度、角度、加速度等等，這部分我們就不細說。\n",
    "\n",
    "### Action\n",
    "\n",
    "而在 agent 得到 observation 和 reward 以後，能夠採取的動作有："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "p1k4dIrBAaKi",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "84ae7e50-8650-4281-c371-26b3317671ea"
   },
   "source": [
    "print(env.action_space)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dejXT6PHBrPn"
   },
   "source": [
    "`Discrete(4)` 說明 agent 可以採取四種離散的行動：\n",
    "- 0 代表不採取任何行動\n",
    "- 2 代表主引擎向下噴射\n",
    "- 1, 3 則是向左右噴射\n",
    "\n",
    "接下來，我們嘗試讓 agent 與 environment 互動。\n",
    "在進行任何操作前，建議先呼叫 `reset()` 函式讓整個「環境」重置。\n",
    "而這個函式同時會回傳「環境」最初始的狀態。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pi4OmrmZgnWA",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a9813cf2-bf4a-411b-ab41-e72231aec3ec"
   },
   "source": [
    "initial_state = env.reset()\n",
    "print(initial_state)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uBx0mEqqgxJ9"
   },
   "source": [
    "接著，我們試著從 agent 的四種行動空間中，隨機採取一個行動"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vxkOEXRKgizt",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "fec7ab12-7f8a-4314-e98a-398f3a9a9a9d"
   },
   "source": [
    "random_action = env.action_space.sample()\n",
    "print(random_action)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mns-bO01g0-J"
   },
   "source": [
    "再利用 `step()` 函式讓 agent 根據我們隨機抽樣出來的 `random_action` 動作。\n",
    "而這個函式會回傳四項資訊：\n",
    "- observation / state\n",
    "- reward\n",
    "- 完成與否\n",
    "- 其餘資訊"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    ""
   ],
   "metadata": {
    "id": "2FrqFfXzUaGS"
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "E_WViSxGgIk9"
   },
   "source": [
    "observation, reward, done, info = env.step(random_action)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FdieGq7NuBIm"
   },
   "source": [
    "第一項資訊 `observation` 即為 agent 採取行動之後，agent 對於環境的 observation 或者說環境的 state 為何。\n",
    "而第三項資訊 `done` 則是 `True` 或 `False` 的布林值，當登月小艇成功著陸或是不幸墜毀時，代表這個回合（episode）也就跟著結束了，此時 `step()` 函式便會回傳 `done = True`，而在那之前，`done` 則保持 `False`。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yK7r126kuCNp",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "02563f76-7a03-473d-ed44-e349a6adb4e5"
   },
   "source": [
    "print(done)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKdS8vOihxhc"
   },
   "source": [
    "### Reward\n",
    "\n",
    "而「環境」給予的 reward 大致是這樣計算：\n",
    "- 小艇墜毀得到 -100 分\n",
    "- 小艇在黃旗幟之間成功著地則得 100~140 分\n",
    "- 噴射主引擎（向下噴火）每次 -0.3 分\n",
    "- 小艇最終完全靜止則再得 100 分\n",
    "- 小艇每隻腳碰觸地面 +10 分\n",
    "\n",
    "> Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points.\n",
    "> If lander moves away from landing pad it loses reward back.\n",
    "> Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points.\n",
    "> Each leg ground contact is +10.\n",
    "> Firing main engine is -0.3 points each frame.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vxQNs77hi0_7",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "891798d3-be35-4092-be90-7cd266ec1123"
   },
   "source": [
    "print(reward) # after doing a random action (0), the immediate reward is stored in this "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mhqp6D-XgHpe"
   },
   "source": [
    "### Random Agent\n",
    "\n",
    "最後，在進入實做之前，我們就來看看這樣一個 random agent 能否成功登陸月球："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Y3G0bxoccelv",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 269
    },
    "outputId": "5cd91a3f-b4b9-41d8-d7e2-a926a035dacb"
   },
   "source": [
    "\n",
    "env.reset()\n",
    "\n",
    "img = plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "done = False\n",
    "while not done:\n",
    "    action = env.action_space.sample()\n",
    "    observation, reward, done, _ = env.step(action)\n",
    "\n",
    "    img.set_data(env.render(mode='rgb_array'))\n",
    "    display.display(plt.gcf())\n",
    "    display.clear_output(wait=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F5paWqo7tWL2"
   },
   "source": [
    "## Policy Gradient\n",
    "\n",
    "現在來搭建一個簡單的 policy network。\n",
    "我們預設模型的輸入是 8-dim 的 observation，輸出則是離散的四個動作之一："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "J8tdmeD-tZew"
   },
   "source": [
    "class PolicyGradientNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(8, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 4)\n",
    "\n",
    "    def forward(self, state):\n",
    "        hid = torch.tanh(self.fc1(state))\n",
    "        hid = torch.tanh(self.fc2(hid))\n",
    "        return F.softmax(self.fc3(hid), dim=-1)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ynbqJrhIFTC3"
   },
   "source": [
    "再來，搭建一個簡單的 agent，並搭配上方的 policy network 來採取行動。\n",
    "這個 agent 能做到以下幾件事：\n",
    "- `learn()`：從記下來的 log probabilities 及 rewards 來更新 policy network。\n",
    "- `sample()`：從 environment 得到 observation 之後，利用 policy network 得出應該採取的行動。\n",
    "而此函式除了回傳抽樣出來的 action，也會回傳此次抽樣的 log probabilities。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zZo-IxJx286z"
   },
   "source": [
    "\n",
    "class PolicyGradientAgent():\n",
    "    \n",
    "    def __init__(self, network):\n",
    "        self.network = network\n",
    "        self.optimizer = optim.SGD(self.network.parameters(), lr=0.001)\n",
    "         \n",
    "    def forward(self, state):\n",
    "        return self.network(state)\n",
    "    def learn(self, log_probs, rewards):\n",
    "        loss = (-log_probs * rewards).sum() # You don't need to revise this to pass simple baseline (but you can)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "    def sample(self, state):\n",
    "        action_prob = self.network(torch.FloatTensor(state))\n",
    "        action_dist = Categorical(action_prob)\n",
    "        action = action_dist.sample()\n",
    "        log_prob = action_dist.log_prob(action)\n",
    "        return action.item(), log_prob\n",
    "\n",
    "    def save(self, PATH): # You should not revise this\n",
    "        Agent_Dict = {\n",
    "            \"network\" : self.network.state_dict(),\n",
    "            \"optimizer\" : self.optimizer.state_dict()\n",
    "        }\n",
    "        torch.save(Agent_Dict, PATH)\n",
    "\n",
    "    def load(self, PATH): # You should not revise this\n",
    "        checkpoint = torch.load(PATH)\n",
    "        self.network.load_state_dict(checkpoint[\"network\"])\n",
    "        #如果要儲存過程或是中斷訓練後想繼續可以用喔 ^_^\n",
    "        self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ehPlnTKyRZf9"
   },
   "source": [
    "最後，建立一個 network 和 agent，就可以開始進行訓練了。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GfJIvML-RYjL"
   },
   "source": [
    "network = PolicyGradientNetwork()\n",
    "agent = PolicyGradientAgent(network)\n",
    "#agent = PolicyGradientAgent()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ouv23glgf5Qt"
   },
   "source": [
    "## 訓練 Agent\n",
    "\n",
    "現在我們開始訓練 agent。\n",
    "透過讓 agent 和 environment 互動，我們記住每一組對應的 log probabilities 及 reward，並在成功登陸或者不幸墜毀後，回放這些「記憶」來訓練 policy network。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "vg5rxBBaf38_",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "a3993442168a4e36a7aaf9503be3cd01",
      "f124e93ecf2e4c5f8d8fd762bd43824f",
      "3b4f60be74394d3a805523bd6f547fa7",
      "d5f7f262acf24a2faaf3cb47cb83d01a",
      "b723620772ec456aaefd9bcf62ecc862",
      "4f945cd083aa433cb8acc84cce252f2d",
      "28e6008640ed42b3b2dc6fea8560f5bb",
      "9425467881314a15821c657b9b1179d8",
      "bca0804664a9448ca3b0861b24bfb5af",
      "a9923d8f5ef14968bfb79e5afdcf90dd",
      "0b47ba37ea5045e2a86be7be362124fe"
     ]
    },
    "outputId": "10f3d131-6a01-4e06-c826-d4913cc818ae"
   },
   "source": [
    "agent.network.train()  # 訓練前，先確保 network 處在 training 模式\n",
    "EPISODE_PER_BATCH = 5  # 每蒐集 5 個 episodes 更新一次 agent\n",
    "NUM_BATCH = 1000        # 總共更新 400 次\n",
    "\n",
    "avg_total_rewards, avg_final_rewards = [], []\n",
    "\n",
    "prg_bar = tqdm(range(NUM_BATCH))\n",
    "for batch in prg_bar:\n",
    "\n",
    "    log_probs, rewards = [], []\n",
    "    total_rewards, final_rewards = [], []\n",
    "\n",
    "    # 蒐集訓練資料\n",
    "    for episode in range(EPISODE_PER_BATCH):\n",
    "        \n",
    "        state = env.reset()\n",
    "        total_reward, total_step = 0, 0\n",
    "        seq_rewards = []\n",
    "        while True:\n",
    "\n",
    "            action, log_prob = agent.sample(state) # at , log(at|st)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            log_probs.append(log_prob) # [log(a1|s1), log(a2|s2), ...., log(at|st)]\n",
    "            # seq_rewards.append(reward)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            total_step += 1\n",
    "            rewards.append(0) #改這裡\n",
    "            count = len(rewards)\n",
    "            for index in range(count):\n",
    "              rewards[index] += 0.99**(count - index - 1 ) * reward\n",
    "\n",
    "            # ! 重要 ！\n",
    "            # 現在的reward 的implementation 為每個時刻的瞬時reward, 給定action_list : a1, a2, a3 ......\n",
    "            #                                                       reward :     r1, r2 ,r3 ......\n",
    "            # medium：將reward調整成accumulative decaying reward, 給定action_list : a1,                         a2,                           a3 ......\n",
    "            #                                                       reward :     r1+0.99*r2+0.99^2*r3+......, r2+0.99*r3+0.99^2*r4+...... ,r3+0.99*r4+0.99^2*r5+ ......\n",
    "            # boss : implement DQN\n",
    "            if done:\n",
    "                final_rewards.append(reward)\n",
    "                total_rewards.append(total_reward)\n",
    "                break\n",
    "\n",
    "    # print(f\"rewards looks like \", np.shape(rewards))  \n",
    "    # print(f\"log_probs looks like \", np.shape(log_probs))     \n",
    "    # 紀錄訓練過程\n",
    "    avg_total_reward = sum(total_rewards) / len(total_rewards)\n",
    "    avg_final_reward = sum(final_rewards) / len(final_rewards)\n",
    "    avg_total_rewards.append(avg_total_reward)\n",
    "    avg_final_rewards.append(avg_final_reward)\n",
    "    prg_bar.set_description(f\"Total: {avg_total_reward: 4.1f}, Final: {avg_final_reward: 4.1f}\")\n",
    "\n",
    "    # 更新網路\n",
    "    # rewards = np.concatenate(rewards, axis=0)\n",
    "    rewards = (rewards - np.mean(rewards)) / (np.std(rewards) + 1e-9)  # 將 reward 正規標準化\n",
    "    agent.learn(torch.stack(log_probs), torch.from_numpy(rewards))\n",
    "    # print(\"logs prob looks like \", torch.stack(log_probs).size())\n",
    "    # print(\"torch.from_numpy(rewards) looks like \", torch.from_numpy(rewards).size())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vNb_tuFYhKVK"
   },
   "source": [
    "### 訓練結果\n",
    "\n",
    "訓練過程中，我們持續記下了 `avg_total_reward`，這個數值代表的是：每次更新 policy network 前，我們讓 agent 玩數個回合（episodes），而這些回合的平均 total rewards 為何。\n",
    "理論上，若是 agent 一直在進步，則所得到的 `avg_total_reward` 也會持續上升，直至 250 上下。\n",
    "若將其畫出來則結果如下："
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wZYOI8H10SHN",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "outputId": "eda908da-48fe-4a1b-e297-5aeed0847fd0"
   },
   "source": [
    "end = time.time()\n",
    "plt.plot(avg_total_rewards)\n",
    "plt.title(\"Total Rewards\")\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mV5jj4dThz0Y"
   },
   "source": [
    "另外，`avg_final_reward` 代表的是多個回合的平均 final rewards，而 final reward 即是 agent 在單一回合中拿到的最後一個 reward。\n",
    "如果同學們還記得環境給予登月小艇 reward 的方式，便會知道，不論**回合的最後**小艇是不幸墜毀、飛出畫面、或是靜止在地面上，都會受到額外地獎勵或處罰。\n",
    "也因此，final reward 可被用來觀察 agent 的「著地」是否順利等資訊。"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "txDZ5vlGWz5w",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 281
    },
    "outputId": "13c7a854-37d6-4df4-e096-5238c6053ff8"
   },
   "source": [
    "plt.plot(avg_final_rewards)\n",
    "plt.title(\"Final Rewards\")\n",
    "plt.show()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gyT7tNwkVdS-"
   },
   "source": [
    "訓練時間\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_t-JsKxUViFy",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "aa61950b-cd2f-45e0-a541-abd5b7328de4"
   },
   "source": [
    "print(f\"total time is {end-start} sec\")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2HaGRVEYGQS"
   },
   "source": [
    "## 測試"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5yFuUKKRYH73",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "outputId": "17db0045-87fb-4b37-cab3-3a1183bbe481"
   },
   "source": [
    "fix(env, seed)\n",
    "agent.network.eval()  # 測試前先將 network 切換為 evaluation 模式\n",
    "NUM_OF_TEST = 5 # Do not revise it !!!!!\n",
    "test_total_reward = []\n",
    "action_list = []\n",
    "for i in range(NUM_OF_TEST):\n",
    "  actions = []\n",
    "  state = env.reset()\n",
    "\n",
    "  img = plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "  total_reward = 0\n",
    "\n",
    "  done = False\n",
    "  while not done:\n",
    "      action, _ = agent.sample(state)\n",
    "      actions.append(action)\n",
    "      state, reward, done, _ = env.step(action)\n",
    "\n",
    "      total_reward += reward\n",
    "\n",
    "      #img.set_data(env.render(mode='rgb_array'))\n",
    "      #display.display(plt.gcf())\n",
    "      #display.clear_output(wait=True)\n",
    "  print(total_reward)\n",
    "  test_total_reward.append(total_reward)\n",
    "\n",
    "  action_list.append(actions) #儲存你測試的結果\n",
    "  print(\"length of actions is \", len(actions))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Aex7mcKr0J01",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "96e65191-34cb-46a7-ade9-4ef7dba73c04"
   },
   "source": [
    "print(f\"Your final reward is : %.2f\"%np.mean(test_total_reward))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leyebGYRpqsF"
   },
   "source": [
    "Action list 的長相"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hGAH4YWDpp4u",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "0eb4df99-8c55-4211-aa1b-330c52456afa"
   },
   "source": [
    "print(\"Action list looks like \", action_list)\n",
    "print(\"Action list's shape looks like \", np.shape(action_list))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7sokqEUtrFY"
   },
   "source": [
    "Action 的分布\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WHdAItjj1nxw",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "2394a27a-cc13-42f0-b132-493bfaeed72c"
   },
   "source": [
    "distribution = {}\n",
    "for actions in action_list:\n",
    "  for action in actions:\n",
    "    if action not in distribution.keys():\n",
    "      distribution[action] = 1\n",
    "    else:\n",
    "      distribution[action] += 1\n",
    "print(distribution)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ricE0schY75M"
   },
   "source": [
    "儲存 Model Testing的結果\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GZsMkGmIY42b",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a2b37602-f5ee-4eff-df1f-0816c1fff0e3"
   },
   "source": [
    "PATH = \"Action_List_test.npy\" # 可以改成你想取的名字或路徑\n",
    "np.save(PATH ,np.array(action_list)) "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "asK7WfbkaLjt"
   },
   "source": [
    "### 你要交到JudgeBoi的檔案94這個\n",
    "儲存結果到本地端 (就是你的電腦裡拉 = = )\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "c-CqyhHzaWAL"
   },
   "source": [
    "from google.colab import files\n",
    "files.download(PATH)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "seT4NUmWmAZ1"
   },
   "source": [
    "# Server 測試\n",
    "到時候下面會是我們Server上測試的環境，可以給大家看一下自己的表現如何"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "U69c-YTxaw6b"
   },
   "source": [
    "action_list = np.load(PATH,allow_pickle=True) #到時候你上傳的檔案\n",
    "seed = 543 #到時候測試的seed 請不要更改\n",
    "fix(env, seed)\n",
    "\n",
    "agent.network.eval()  # 測試前先將 network 切換為 evaluation 模式\n",
    "\n",
    "test_total_reward = []\n",
    "for actions in action_list:\n",
    "  state = env.reset()\n",
    "  img = plt.imshow(env.render(mode='rgb_array'))\n",
    "\n",
    "  total_reward = 0\n",
    "\n",
    "  done = False\n",
    "  # while not done:\n",
    "  done_count = 0\n",
    "  for action in actions:\n",
    "      # action, _ = agent1.sample(state)\n",
    "      state, reward, done, _ = env.step(action)\n",
    "      done_count += 1\n",
    "      total_reward += reward\n",
    "      if done:\n",
    "        \n",
    "        break\n",
    "    #   img.set_data(env.render(mode='rgb_array'))\n",
    "    #   display.display(plt.gcf())\n",
    "    #   display.clear_output(wait=True)\n",
    "  print(f\"Your reward is : %.2f\"%total_reward)\n",
    "  test_total_reward.append(total_reward)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjFBWwQP1hVe"
   },
   "source": [
    "# 你的成績"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GpJpZz3Wbm0X"
   },
   "source": [
    "print(f\"Your final reward is : %.2f\"%np.mean(test_total_reward))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUBtYXG2eaqf"
   },
   "source": [
    "## 參考資料\n",
    "\n",
    "以下是一些有用的參考資料。\n",
    "建議同學們實做前，可以先參考第一則連結的上課影片。\n",
    "在影片的最後有提到兩個有用的 Tips，這對於本次作業的實做非常有幫助。\n",
    "\n",
    "- [DRL Lecture 1: Policy Gradient (Review)](https://youtu.be/z95ZYgPgXOY)\n",
    "- [ML Lecture 23-3: Reinforcement Learning (including Q-learning) start at 30:00](https://youtu.be/2-JNBzCq77c?t=1800)\n",
    "- [Lecture 7: Policy Gradient, David Silver](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/pg.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGqP2EU1joWM"
   },
   "source": [
    ""
   ]
  }
 ]
}